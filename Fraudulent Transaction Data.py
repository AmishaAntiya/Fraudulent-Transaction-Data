# -*- coding: utf-8 -*-
"""KDD Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHWD5BjdytIKHkGTMvAm0RH7mY-d51Rb
"""

# Importing library

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Reading Fraud dataset
df = pd.read_csv('Fraud.csv')
df.head()

# Dataset description
df.info()

"""Data Dictionary:

**step** - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).

**type** - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.

**amount** - amount of the transaction in local currency.

**nameOrig** - customer who started the transaction

**oldbalanceOrg** - initial balance before the transaction

**newbalanceOrig** - new balance after the transaction

**nameDest** - customer who is the recipient of the transaction

**oldbalanceDest** - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).

**newbalanceDest** - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).

**isFraud** - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.

**isFlaggedFraud** - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.
"""

df.info()

df.describe()

# Defining target
target = 'isFraud'

plt.figure(figsize=[7, 7])
sns.countplot(df[target]);

df[target].value_counts()

"""Target Variable is highly Imbalanced

# Distribution of categorical features
"""

df.type.unique()

sns.countplot(df.type)

# Analyze number of fraud transations in each of the features
df.groupby([target, 'type']).size().unstack(fill_value=0)

"""There are no fraud transactions in CASH_IN, DEBIT, PAYMENT

nameOrig feature is just customer name/ID which makes it pretty irrelevent when training our models, same is probably true for nameDest
"""

fraud = df[df['isFraud'] == 1]

sns.distplot(df['amount'])

sns.distplot(fraud['amount']);

"""We can see that fraud is only when amount is low."""

types = pd.get_dummies(df['type'], prefix='type', drop_first=True)
types.head()

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()

df['nameOrig'] = label.fit_transform(df['nameOrig'])
df['nameDest'] = label.fit_transform(df['nameDest'])

df.head()

df = pd.concat([df, types], axis=1)
df = df.drop('type', axis=1)
df.head()

plt.figure(figsize=[12, 10])

sns.heatmap(df.corr(), annot=True, vmax=1, vmin=-1)
plt.show()

"""# Splitting the dataset into training set and test set"""

from sklearn.model_selection import train_test_split

# Seperate features and target
X = df.drop('isFraud', axis=1)
y = df['isFraud']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=32)

"""**Standardizing the values**"""

from sklearn.preprocessing import StandardScaler
std = StandardScaler()

# Standardizing the training set
X_train_std = X_train
X_train_std = std.fit_transform(X_train_std)

# Standardizing the testing set
X_test_std = X_test
X_test_std = std.transform(X_test_std)

y_train

X_train_std

"""# 1. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=3, class_weight='balanced', n_jobs=-1)
rf.fit(X_train_std, y_train)

from sklearn.metrics import f1_score, classification_report

pred = rf.predict(X_test_std)

# F1 score
rf_score = f1_score(y_test, pred)
print(rf_score)

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')

"""# 2. Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train_std, y_train)

pred = gnb.predict(X_test_std)

# F1 score
nb_score = f1_score(y_test, pred)
print(nb_score)

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')

"""# 3. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(criterion="gini")
dt.fit(X_train_std, y_train)

pred = dt.predict(X_test_std)

# F1 score
dt_score = f1_score(y_test, pred)
print(dt_score)

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')

"""# 4. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train_std, y_train)

pred = lr.predict(X_test_std)

# F1 score
lr_score = f1_score(y_test, pred)
print(lr_score)

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')

"""# 5. K-nearest neighbors"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_std, y_train)

pred = knn.predict(X_test_std)

# F1 score
knn_score = f1_score(y_test, pred)
print(knn_score)

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')

"""# 6. Artificial Neural Network (ANN)"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers.normalization.batch_normalization import BatchNormalization

# Creating 5 Layer
classifier = Sequential()
classifier.add(Dense(5,kernel_initializer='he_uniform',activation='relu',input_dim=X_train_std.shape[1]))
classifier.add(Flatten())
classifier.add(Dense(1,kernel_initializer='glorot_uniform',activation='sigmoid'))

# Summary
classifier.summary()

# Compiling
import tensorflow
from tensorflow.keras.optimizers import Adam

learning_rate = 0.001
opt = Adam(learning_rate=learning_rate)
classifier.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])

# Fitting ANN into training set
model = classifier.fit(X_train_std, y_train, batch_size=100, epochs=35)

# Predicting data
pred = classifier.predict(X_test_std)

# Converting to binary data
pred = (pred > 0.5)

# F1 score
print(f1_score(y_test, pred))

# Classification report
print(classification_report(y_test, pred))

from sklearn.metrics import accuracy_score

print(f'Test Accuracy:- {accuracy_score(y_test, pred)*100}%')